{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as rq\n",
    "import re\n",
    "import pandas as pd\n",
    "!pip install selenium \n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Scrape the details of most viewed videos on YouTube from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading contents of the web page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "data = rq.get(url)\n",
    "html_data = data.content\n",
    "\n",
    "# Creating BeautifulSoup object\n",
    "soup = bs(html_data, 'lxml')\n",
    "\n",
    "# Creating list with all tables\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "#  Looking for the table with the classes 'wikitable' and 'sortable'\n",
    "table = soup.find('table', class_='wikitable sortable')\n",
    "\n",
    "#Creating lists\n",
    "row = []\n",
    "\n",
    "#creating loop for extracting data in each row\n",
    "for line in table.tbody.find_all('tr'):    \n",
    "    # Find all data for each column\n",
    "    columns = line.find_all('td')\n",
    "    \n",
    "    if(columns != []):\n",
    "        Rank1 = columns[0].text.strip()\n",
    "        Name1 = columns[1].text.strip().split('\"')[1].strip()\n",
    "        Artist1 = columns[2].text.strip()\n",
    "        Views1 = columns[3].text.strip()\n",
    "        Upload1 = columns[4].text.strip()\n",
    "                \n",
    "    row.append([Rank1,Name1,Artist1,Views1,Upload1])\n",
    "    \n",
    "df = pd.DataFrame(row,columns = ['Rank','Name','Artist','Views','Upload'])\n",
    "df.sort_values(by=['Views'])\n",
    "df.drop_duplicates(subset=\"Name\", keep='last', inplace=True)        \n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the automated chrome driver\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Vinit\\Downloads\\chromedriver.exe')\n",
    "\n",
    "\n",
    "#URL to be web scrape\n",
    "\n",
    "url = 'https://www.bcci.tv/'\n",
    "driver.get(url)\n",
    "\n",
    "#clicking the search button\n",
    "fixture_button = driver.find_element(By.XPATH,\"//div[@id='navigation']/ul[1]/li[2]/a\")\n",
    "fixture_button.click()\n",
    "time.sleep(10)\n",
    "\n",
    "#morefixture_button.click()\n",
    "morefixture_button = driver.find_element(By.XPATH,\"//button[@class='match-btn btn-red d-flex align-items-center justify-content-center mx-auto mt-3']\")\n",
    "driver.execute_script(\"arguments[0].click();\", morefixture_button)\n",
    "\n",
    "\n",
    "fixture=driver.find_elements(By.XPATH,\"//div[@class='fixture-card-main col-lg-3 col-md-6 col-sm-12 ng-scope']\")\n",
    "#Creating dictionary\n",
    "date =[]\n",
    "times = []\n",
    "series = []\n",
    "title = []\n",
    "place = []\n",
    "\n",
    "#extracting data match by match\n",
    "for i in range(len(fixture)):\n",
    "    dates= driver.find_elements(By.XPATH, \"//*[@id='fixtures']/div[3]/div/div/div/div[1]/div/div[1]/h5\")\n",
    "    date.append(dates[i].text)\n",
    "    time1= driver.find_elements(By.XPATH, \"//h5[@class='text-right ng-binding']\")\n",
    "    times.append(time1[i].text)\n",
    "    series1= driver.find_elements(By.XPATH,\"//span[@class='ng-binding']\")\n",
    "    series.append(series1[i].text)\n",
    "    titles = driver.find_elements(By.XPATH,\"//span[@class='matchOrderText ng-binding ng-scope']\")\n",
    "    title.append(titles[i].text)\n",
    "    places =driver.find_elements(By.XPATH,\"//span[@class='ng-binding ng-scope']\")\n",
    "    place.append(places[i].text)\n",
    "\n",
    "#Creating dataframe\n",
    "fixtData = {'title':title,'series':series,'place':place,'date':date,'time':times}\n",
    "        \n",
    "df = pd.DataFrame.from_dict(fixtData)\n",
    "df.head(10)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Scrape the details of selenium exception from guru99.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the automated chrome driver\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Vinit\\Downloads\\chromedriver.exe')\n",
    "\n",
    "\n",
    "#URL to be web scrape\n",
    "\n",
    "url = 'https://www.guru99.com/'\n",
    "driver.get(url)\n",
    "driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "time.sleep(50)\n",
    "\n",
    "#Search for selenium exception\n",
    "search_selenium = driver.find_element(By.XPATH,\"//input[@class='gsc-input']\")\n",
    "search_selenium.send_keys('selenium exception handling')\n",
    "\n",
    "\n",
    "#clicking the search button\n",
    "search_button = driver.find_element(By.XPATH,\"//button[@class='gsc-search-button gsc-search-button-v2']\")\n",
    "search_button.click()\n",
    "time.sleep(5)\n",
    "    \n",
    "search_result=driver.find_element(By.XPATH,\"(//div[@class='gs-title']/a)[1]\")\n",
    "resultURL = search_result.get_attribute('href')\n",
    "driver.get(resultURL)\n",
    "time.sleep(10)\n",
    "    \n",
    "title= []\n",
    "discription = []\n",
    "\n",
    "titleTags =driver.find_elements(By.XPATH,\"//article[@id='post-1953']/div/div/p/strong\")\n",
    "discrpTags =driver.find_elements(By.XPATH,\"//article[@id='post-1953']/div/div/p\")\n",
    "\n",
    "for i in range(41):\n",
    "    title.append(titleTags[i].text.split('.')[1].split(':')[0])\n",
    "    discription.append(discrpTags[i].text.split(':')[1])   \n",
    "\n",
    "    \n",
    "dictionary= {'Title of Exception':title,'Discription':discription}\n",
    "driver.close()\n",
    "df = pd.DataFrame.from_dict(dictionary)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scrape the details of State-wise GDP of India from statisticstime.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the automated chrome driver\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Vinit\\Downloads\\chromedriver.exe')\n",
    "\n",
    "\n",
    "#URL to be web scrape\n",
    "\n",
    "url = 'https://www.statisticstimes.com/'\n",
    "driver.get(url)\n",
    "\n",
    "economyButton = driver.find_element(By.XPATH,\"//div[@id='top']/div[2]/div[2]/div[@class='dropdown-content']/a[3]\")\n",
    "URL=economyButton.get_attribute('href')\n",
    "\n",
    "driver.get(URL)\n",
    "\n",
    "stateGDP = driver.find_element(By.XPATH,\"//html/body/div[2]/div[2]/div[2]/ul/li[1]/a\")\n",
    "finalURL = stateGDP.get_attribute('href')\n",
    "\n",
    "driver.get(finalURL)\n",
    "\n",
    "# Downloading contents of the web page\n",
    "data = rq.get(finalURL)\n",
    "html_data = data.content\n",
    "\n",
    "# Creating BeautifulSoup object\n",
    "soup = bs(html_data, 'lxml')\n",
    "\n",
    "# Creating list with all tables\n",
    "#tables = soup.find_all('table',id=\"table_id\")\n",
    "table = soup.find( \"table\", {\"id\":\"table_id\"} )\n",
    "\n",
    "#Creating lists\n",
    "row = []\n",
    "\n",
    "#creating loop for extracting data in each row\n",
    "for line in table.tbody.find_all('tr'):    \n",
    "    # Find all data for each column\n",
    "    columns = line.find_all('td')\n",
    "    if(columns != []):\n",
    "        Rank1 = columns[0].text.strip()\n",
    "        stateName = columns[1].text.strip()\n",
    "        GSDP19 = columns[2].text.strip()\n",
    "        GSDP18 = columns[3].text.strip()\n",
    "        share = columns[4].text.strip()\n",
    "        gdp=columns[5].text.strip()\n",
    "    row.append([Rank1,stateName,GSDP19,GSDP18,share,gdp])\n",
    "    \n",
    "df = pd.DataFrame(row,columns = ['Rank','State','GSDP -Cr INR at Current prices 2019-20',\n",
    "                                 'GSDP -Cr INR at Current prices 2018-19','Share','GDP in Billion'])       \n",
    "df.head(20)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Scrape the details of top 100 songs on billiboard.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the automated chrome driver\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Vinit\\Downloads\\chromedriver.exe')\n",
    "\n",
    "\n",
    "#URL to be web scrape\n",
    "\n",
    "url = 'https://www.billboard.com/'\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "    \n",
    "chartmenu = driver.find_element(By.XPATH,\"//div[@id='main-wrapper']/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a\")\n",
    "URL=chartmenu.get_attribute('href')\n",
    "driver.get(URL)\n",
    "time.sleep(5)\n",
    "\n",
    "viewchart = driver.find_element(By.XPATH,\"//div[@id='main-wrapper']/main/div[2]/div[1]/div[1]/div/div/div[3]/a\")\n",
    "finalURL = viewchart.get_attribute('href')\n",
    "driver.get(finalURL)\n",
    "time.sleep(5)\n",
    "\n",
    "rowtag = driver.find_elements(By.XPATH,\"//div[@class='o-chart-results-list-row-container']\")\n",
    "RANKS = []\n",
    "SONGS = []\n",
    "ARTISTS = []\n",
    "LASTWEEKS = []\n",
    "PEAKS = []\n",
    "WEEKSONCHART = []\n",
    "for i in range (len(rowtag)):\n",
    "    rank = driver.find_elements(By.XPATH,\"//div/div[2]/div/ul/li[1]/span[@class='c-label  a-font-primary-bold-l u-font-size-32@tablet u-letter-spacing-0080@tablet']\")\n",
    "    RANKS.append(rank[i].text)\n",
    "    song = driver.find_elements(By.XPATH,\"//li/h3[@id='title-of-a-story']\")\n",
    "    SONGS.append(song[i].text)\n",
    "    artist = driver.find_elements(By.XPATH,\"//div[2]/div/ul/li[4]/ul/li[1]/span\")\n",
    "    ARTISTS.append(artist[i].text)\n",
    "    lastweek = driver.find_elements(By.XPATH,\"//div/div[2]/div/ul/li[4]/ul/li[4]/span\")\n",
    "    LASTWEEKS.append(lastweek[i].text)\n",
    "    peak = driver.find_elements(By.XPATH,\"//div[2]/div/ul/li[4]/ul/li[5]/span\")\n",
    "    PEAKS.append(peak[i].text)\n",
    "    weeksOnchart = driver.find_elements(By.XPATH,\"//div/div[2]/div/ul/li[4]/ul/li[6]/span\")\n",
    "    WEEKSONCHART.append(weeksOnchart[i].text)\n",
    "    \n",
    "dictionary= {'Rank':RANKS,'Song':SONGS,'Artist':ARTISTS,'Last Week Rank':LASTWEEKS,'Peak Rank':PEAKS,'Weeks on Chart':WEEKSONCHART}\n",
    "driver.close()\n",
    "df = pd.DataFrame.from_dict(dictionary)\n",
    "df = df.set_index('Rank')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Scrape the details of Data science recruiters from naukri.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the automated chrome driver\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Vinit\\Downloads\\chromedriver.exe')\n",
    "\n",
    "\n",
    "#URL to be web scrape\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "#Search Data science jobs\n",
    "search = driver.find_element(By.XPATH,\"//*[@id='root']/div[6]/div/div/div[1]/div/div/div/input\")\n",
    "search.send_keys('Data Science')\n",
    "\n",
    "\n",
    "#clicking the search button\n",
    "search_button = driver.find_element(By.XPATH,\"//*[@id='root']/div[6]/div/div/div[6]\")\n",
    "search_button.click()\n",
    "time.sleep(10)\n",
    "\n",
    "results = driver.find_elements(By.XPATH,\"//*[@id='root']/div[4]/div/div/section[2]/div[2]/article\")\n",
    "\n",
    "def listToString(tra):\n",
    "   \n",
    "    # initialize an empty string\n",
    "    str1 = \", \"\n",
    "   \n",
    "    # return string \n",
    "    return (str1.join(tra))\n",
    "\n",
    "Name = []\n",
    "Designation =[]\n",
    "Company = []\n",
    "Skills = []\n",
    "Location = []\n",
    "\n",
    "\n",
    "for i in range(len(results)):\n",
    "    \n",
    "    Title = driver.find_elements(By.XPATH,\"//article/div[1]/div[1]/a\")\n",
    "    try:\n",
    "        Designation.append(Title[i].text.split('-')[1])\n",
    "        Name.append(Title[i].text.split('-')[0])\n",
    "        \n",
    "    except:\n",
    "        Name.append(Title[i].text)\n",
    "        Designation.append('-')\n",
    "\n",
    "    org = driver.find_elements(By.XPATH,\"//article/div[1]/div[1]/div/a[1]\")\n",
    "    Company.append(org[i].text)\n",
    "\n",
    "    skillpath = (\"//article\"+str([i])+\"/ul/li\")\n",
    "    Skill = driver.find_elements(By.XPATH,skillpath)\n",
    "    for s in range(len(Skill)):\n",
    "        tra.append(Skill[s].text)\n",
    "    Skills.append(listToString(tra))\n",
    "    tra = []\n",
    "    \n",
    "    location = driver.find_elements(By.XPATH,\"//div[1]/div[1]/ul/li[3]/span\")\n",
    "    Location.append(location[i].text)\n",
    "    \n",
    "    \n",
    "dictionary= {'Job Title':Name,'Designation':Designation,'Comapny':Company,'Skills':Skills,'Location':Location}\n",
    "driver.close()\n",
    "df = pd.DataFrame.from_dict(dictionary)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Scrape the details most watched tv series of all time from imdb.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the automated chrome driver\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Vinit\\Downloads\\chromedriver.exe')\n",
    "\n",
    "\n",
    "#URL to be web scrape\n",
    "\n",
    "url = 'https://www.imdb.com/list/ls095964455/'\n",
    "driver.get(url)\n",
    "\n",
    "results = driver.find_elements(By.XPATH,\"//div[@id='main']/div/div[3]/div[3]/div\")\n",
    "\n",
    "Name =[]\n",
    "Year = []\n",
    "RunTime = []\n",
    "Genere = []\n",
    "Rating = []\n",
    "Vote = []\n",
    "\n",
    "for i in range(len(results)):  \n",
    "    nametag = driver.find_elements(By.XPATH,\"//div[@id='main']/div/div[3]/div[3]/div/div[2]/h3/a\")\n",
    "    Name.append(nametag[i].text)\n",
    "    yeartag = driver.find_elements(By.XPATH,\"//div[@id='main']/div/div[3]/div[3]/div/div[2]/h3/span[2]\")\n",
    "    Year.append(yeartag[i].text)\n",
    "    timetag = driver.find_elements(By.XPATH,\"//div[@id='main']/div/div[3]/div[3]/div/div[2]/p[1]/span[3]\")\n",
    "    RunTime.append(timetag[i].text)\n",
    "    generetag = driver.find_elements(By.XPATH,\"//div[@id='main']/div/div[3]/div[3]/div/div[2]/p[1]/span[5]\")\n",
    "    Genere.append(generetag[i].text)\n",
    "    ratingtag = driver.find_elements(By.XPATH,\"//div[@id='main']/div/div[3]/div[3]/div/div[2]/div[1]/div[1]/span[2]\")\n",
    "    Rating.append(ratingtag[i].text)\n",
    "    votetag = driver.find_elements(By.XPATH,\"//div[@id='main']/div/div[3]/div[3]/div/div[2]/p[4]/span[2]\")\n",
    "    Vote.append(votetag[i].text)\n",
    "\n",
    "    \n",
    "dictionary= {'Movie Name':Name,'Year Span':Year,'Run Time in Min.':RunTime,'Genre': Genere,'Rating':Rating,'Votes':Vote}\n",
    "driver.close()\n",
    "df = pd.DataFrame.from_dict(dictionary)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. Details of Datasets from UCI machine learning repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the automated chrome driver\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Vinit\\Downloads\\chromedriver.exe')\n",
    "\n",
    "\n",
    "#URL to be web scrape\n",
    "url = 'https://archive.ics.uci.edu/'\n",
    "\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "\n",
    "viewButton = driver.find_element(By.XPATH,\"/html/body/table[2]/tbody/tr/td/span/b/a\")\n",
    "URL=viewButton.get_attribute('href')\n",
    "\n",
    "driver.get(URL)\n",
    "time.sleep(10)\n",
    "\n",
    "names =[]\n",
    "datatypes=[]\n",
    "default_tasks =[]\n",
    "attritypes =[]\n",
    "intances =[]\n",
    "attributes =[]\n",
    "years=[]\n",
    "\n",
    "rowscount = driver.find_elements(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr\")\n",
    "\n",
    "#for i in range(len(rowscount)):\n",
    "for i in range(5):    \n",
    "    try:\n",
    "        name = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[1]/table/tbody/tr/td[2]/p/b/a')\n",
    "        names.append(name[i].text)\n",
    "    except:\n",
    "        names.insert(1,'Name')\n",
    "    datatype=driver.find_elements(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]/p\")\n",
    "    datatypes.append(datatype[i].text)\n",
    "    default_task=driver.find_elements(By.XPATH, \"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]/p\")\n",
    "    default_tasks.append(default_task[i].text)\n",
    "    attritype=driver.find_elements(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]/p\")\n",
    "    attritypes.append(attritype[i].text)\n",
    "    intance=driver.find_elements(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]/p\")\n",
    "    intances.append(intance[i].text)\n",
    "    attribute=driver.find_elements(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]/p\")\n",
    "    attributes.append(attribute[i].text)\n",
    "    year=driver.find_elements(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]/p\")\n",
    "    years.append(year[i].text)\n",
    "    \n",
    "dictionary = {'Dataset name':names[1:],'Data type':datatypes[1:],'Task':default_tasks[1:],'Attribute type':attritypes[1:],\n",
    "             'No of instances':intances[1:],'No of attribute':attributes[1:],'year':years[1:]}\n",
    "\n",
    "driver.close()\n",
    "df = pd.DataFrame.from_dict(dictionary)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
